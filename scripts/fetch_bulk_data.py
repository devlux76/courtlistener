"""
Bulk Data Fetcher Script

Downloads and extracts the latest bulk data files from the S3 bucket.
Features:
- Multithreaded downloads
- Robust error handling and retry logic
- File size validation before and after download
- Progress tracking
- Atomic file operations to prevent corruption
- No import functionality (download/extract only)
"""

"""
Bulk Data Fetcher Script

Downloads and extracts the latest bulk data files from the S3 bucket.

Features:
- Multithreaded downloads
- Robust error handling and retry logic
- File size validation before and after download
- Progress tracking
- Atomic file operations to prevent corruption
- No import functionality (download/extract only)
"""

import sys
import os
import re
import concurrent.futures
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import requests
import bz2
import time
import tempfile
import shutil

URL = "https://com-courtlistener-storage.s3-us-west-2.amazonaws.com/list.html?prefix=bulk-data/"

def get_links():
    """
    Uses Selenium to fetch all bulk-data file links from the S3 bucket listing page.
    Waits for the links to be generated by JavaScript.

    Returns:
        list of str: URLs to bulk-data files.
    """
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    driver = webdriver.Chrome(options=chrome_options)
    driver.get(URL)
    try:
        # Wait up to 30 seconds for at least one link to appear
        WebDriverWait(driver, 30).until(
            EC.presence_of_element_located((By.XPATH, '//a[contains(@href, "bulk-data/")]'))
        )
        links = [a.get_attribute('href') for a in driver.find_elements(By.XPATH, '//a[contains(@href, "bulk-data/")]')]
    finally:
        driver.quit()
    return links

def get_latest_files_with_sizes(links):
    """
    Given a list of links, finds the latest file for each base name (ignoring .delta files),
    and fetches their sizes via HEAD requests.

    Args:
        links (list of str): URLs to bulk-data files.

    Returns:
        list of (str, str, int): Tuples of (filename, url, size) for latest files, sorted by size descending.
    """
    import requests
    file_map = {}
    for link in links:
        fname = link.split('/')[-1]
        if not fname or fname.endswith('.delta'):
            continue
        base = re.sub(r'-\d{4}-\d{2}-\d{2}', '', fname)
        try:
            head = requests.head(link, timeout=30)
            size = int(head.headers.get("Content-Length", 0))
        except Exception:
            size = 0
        if base not in file_map or fname > file_map[base][0]:
            file_map[base] = (fname, link, size)
    # Sort by size descending
    return sorted(file_map.values(), key=lambda x: x[2], reverse=True)

import time
import tempfile
import shutil

def download_and_extract(item, outdir, max_retries=5):
    """
    Downloads a file from the given URL with retries, validates file size, shows progress,
    writes atomically, and extracts if .bz2.
    All downloads and temporary files are stored in /srv/bulk-data.

    Args:
        item (tuple): (filename, url)
        outdir (str): Output directory (should be /srv/bulk-data)
        max_retries (int): Number of download attempts before giving up
    """
    fname, url = item
    local = os.path.join(outdir, fname)
    target_uncompressed = local.replace('.bz2', '')
    target_csv = local.replace('.bz2', '.csv')
    if os.path.exists(target_uncompressed) or os.path.exists(target_csv):
        print(f"Skipping {fname}, already exists.")
        return

    attempt = 0
    while attempt < max_retries:
        print(f"Downloading {fname} (attempt {attempt + 1}) ...")
        try:
            # HEAD request for file size
            head = requests.head(url, timeout=30)
            head.raise_for_status()
            expected_size = int(head.headers.get("Content-Length", 0))
            if expected_size == 0:
                print(f"Warning: Could not determine file size for {fname}")

            # Download with progress and atomic write
            with tempfile.NamedTemporaryFile(delete=False, dir=outdir) as tmpf:
                tmp_path = tmpf.name
                r = requests.get(url, stream=True, timeout=60)
                r.raise_for_status()
                downloaded = 0
                last_percent = -1
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        tmpf.write(chunk)
                        downloaded += len(chunk)
                        if expected_size:
                            percent = int(downloaded * 100 / expected_size)
                            if percent != last_percent and percent % 10 == 0:
                                print(f"  {percent}% downloaded...", end="\r")
                                last_percent = percent
                tmpf.flush()
            # Validate file size
            actual_size = os.path.getsize(tmp_path)
            if expected_size and actual_size != expected_size:
                raise Exception(f"File size mismatch for {fname}: expected {expected_size}, got {actual_size}")

            # Atomic move
            shutil.move(tmp_path, local)
            print(f"Downloaded {fname} ({actual_size} bytes)")

            # Extraction
            if fname.endswith('.bz2'):
                print(f"Extracting {fname} ...")
                with bz2.open(local, 'rb') as f_in, open(local[:-4], 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
                os.remove(local)
            return
        except Exception as e:
            print(f"Error downloading {fname} (attempt {attempt + 1}): {e}")
            attempt += 1
            if attempt < max_retries:
                wait_time = 2 ** attempt
                print(f"Retrying in {wait_time} seconds...")
                time.sleep(wait_time)
            else:
                print(f"Failed to download {fname} after {max_retries} attempts.")

def main():
    import argparse

    parser = argparse.ArgumentParser(
        description="Fetch and extract latest bulk data files. All downloads and temporary files are stored in /srv/bulk-data."
    )
    parser.add_argument("--workers", type=int, default=4, help="Number of parallel downloads")
    args = parser.parse_args()

    outdir = "/srv/bulk-data"
    os.makedirs(outdir, exist_ok=True)
    print(f"Fetching bulk data into {outdir}")

    links = get_links()
    latest_files = get_latest_files_with_sizes(links)
    print(f"Found {len(latest_files)} files to download (sorted by size).")

    # Disk space check before starting
    def get_available_disk_space(directory):
        statvfs = os.statvfs(directory)
        return statvfs.f_frsize * statvfs.f_bavail

    min_required = max([size for _, _, size in latest_files] + [0])
    available = get_available_disk_space(outdir)
    if available < min_required:
        print(f"Warning: Not enough disk space for the largest file ({min_required} bytes required, {available} available).")

    # Download and extract in parallel, but maintain size order
    def download_and_extract_wrapper(item):
        fname, url, size = item
        # Check disk space before each download
        if get_available_disk_space(outdir) < size:
            print(f"Skipping {fname}: not enough disk space for this file.")
            return
        download_and_extract((fname, url), outdir)

    with concurrent.futures.ThreadPoolExecutor(max_workers=args.workers) as executor:
        futures = [executor.submit(download_and_extract_wrapper, item) for item in latest_files]
        for future in concurrent.futures.as_completed(futures):
            pass

    print("Bulk data fetch complete.")

if __name__ == "__main__":
    main()